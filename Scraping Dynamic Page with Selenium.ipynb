{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Dynamic Pages with Selenium\n",
    "\n",
    "## In order to scrap content generated by JS from dynamic page BeautifulSoup is not enough, since it does not wait for elements that are loaded with the use of JS. However, in order to solve this problem we can use Selenium. There are a few things that we need to take into account though, for example: the one that most often occurs, loading page time (Time out error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First wee need to import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before scrapping we need to make sure that none bug will occur. The first error, which I encoutered is \"ERROR:ssl_client_socket_openssl.cc(1158)] handshake failed with ChromeDriver Chrome browser and Selenium\". The solution for this problem with explanation I found [here](https://stackoverflow.com/questions/37883759/errorssl-client-socket-openssl-cc1158-handshake-failed-with-chromedriver-chr)\n",
    "\n",
    "### \"You get this error when the browser asks you to accept the certificate from the website. You can set to ignore these errors by default in order avoid these errors.\"\n",
    "### You can find the soltuion for Chrome Browser below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--ignore-ssl-errors')\n",
    "driver_path = 'C:\\chromedriver'\n",
    "driver = webdriver.Chrome(driver_path,chrome_options=options)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example below shows the website that shows their articles on different numeric pages, which follows the pattern 0,8,16,24, etc. Therefore, the link f\"https://www.insertherelinktowebsite.com?start={page}\" contains page variable, which will be changing every time the loops begins. This is only an example, each link might differ, therefore, one has to keep that in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start our \"scraping loop\" we gonna create/open csv file, in which we will save the data after every loop finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need the varbiale page later, for our scrapping loop to access another page of a website\n",
    "page = 0\n",
    "with open ('scraped_data.csv', 'w', encoding='utf-8', newline='') as outfile:\n",
    "    writer=csv.writer(outfile)\n",
    "    writer.writerow(['Title','Date','Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I decided to do while loop, since I don't know the exact range of pages that I want to access, my goal is to scrap all articles from 2019, therefore, I use while True, and break it when the program finds 2018 in date\n",
    "### In order to handle errors we use try, under which we insert our code, starting from declaring dictionary to which keys we gonna append information from each page (note that while loop has to be nested inside with open..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        try:\n",
    "            articles_dict = {\n",
    "                    'Title':[],\n",
    "                    'Comment': [],\n",
    "                    'Date': []\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we define the home_url that we gonna scrap\n",
    "### We also make the program to open home_url and make our program to wait 20 seconds (driver.implicitly_wait(20)) before throwing timeout error (more about it below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_url = f\"https://www.insertherelinktowebsite.com?start={page}\"\n",
    "            driver.get(home_url)\n",
    "            driver.implicitly_wait(20)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            page += 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another step is to make our program to write the Comemnts, Titles, Dates to the file that we opened in previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to csv file\n",
    "\n",
    "comments = soup.find_all('span', class_=\"disqus-comment-count\")\n",
    "for comment in comments:\n",
    "    articles_dict['Comment'].append(comment.text)\n",
    "\n",
    "titles = soup.find_all('a', class_='contentpagetitle')\n",
    "for title in titles:\n",
    "    articles_dict['Title'].append(title.text.strip('\\n\\t'))\n",
    "    \n",
    "dates = soup.find_all('span', class_='createdate')\n",
    "for date in dates:\n",
    "    articles_dict['Date'].append(date.text)\n",
    "\n",
    "for i in range(len(articles_dict['Title'])):\n",
    "    writer.writerow([articles_dict['Title'][i],\n",
    "                    articles_dict['Date'][i],\n",
    "                    articles_dict['Comment'][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last step is to instruct our program what to do in case of exceptions\n",
    "### Here we also check if 2018 is in the first article on the page, if yes, then we break the loop and finish the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling exceptions\n",
    "except TimeoutException:\n",
    "# If the loading took too long, print message, refresh and try again\n",
    "    print(\"Loading took too much time!\")\n",
    "    driver.refresh()\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "    print(str(e))\n",
    "except requests.Timeout as e:\n",
    "    print(\"OOPS!! Timeout Error\")\n",
    "    print(str(e))\n",
    "except requests.RequestException as e:\n",
    "    print(\"OOPS!! General Error\")\n",
    "    print(str(e))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Someone closed the program\")\n",
    "\n",
    "if '2018' in dates[0]:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The whole code presents as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--ignore-ssl-errors')\n",
    "driver_path = 'C:\\chromedriver'\n",
    "driver = webdriver.Chrome(driver_path,chrome_options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "page = 0\n",
    "\n",
    "# We can change w to a, if we want to append to already existing data\n",
    "with open ('scraped_data.csv', 'w', encoding='utf-8', newline='') as outfile:\n",
    "    writer=csv.writer(outfile)\n",
    "    writer.writerow(['Title','Date','Comment'])\n",
    "    while True:\n",
    "        try:\n",
    "            articles_dict = {\n",
    "                    'Title':[],\n",
    "                    'Comment': [],\n",
    "                    'Date': []\n",
    "                }\n",
    "\n",
    "            home_url = f\"https://www.insertherelinktowebsite.com?start={page}\"\n",
    "            driver.get(home_url)\n",
    "            driver.implicitly_wait(20)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            instalki_pages.append(soup)\n",
    "            page += 8\n",
    "\n",
    "            # Writing to csv file\n",
    "\n",
    "            comments = soup.find_all('span', class_=\"disqus-comment-count\")\n",
    "            for comment in comments:\n",
    "                articles_dict['Comment'].append(comment.text)\n",
    "\n",
    "            titles = soup.find_all('a', class_='contentpagetitle')\n",
    "            for title in titles:\n",
    "                articles_dict['Title'].append(title.text.strip('\\n\\t'))\n",
    "\n",
    "            for date in dates:\n",
    "                articles_dict['Date'].append(date.text)\n",
    "\n",
    "            for i in range(len(articles_dict['Title'])):\n",
    "                writer.writerow([articles_dict['Title'][i],\n",
    "                                articles_dict['Date'][i],\n",
    "                                articles_dict['Comment'][i]])\n",
    "\n",
    "        # Handling exceptions\n",
    "        except TimeoutException:\n",
    "        # If the loading took too long, print message and try again\n",
    "            print(\"Loading took too much time!\")\n",
    "            driver.refresh()\n",
    "        except requests.ConnectionError as e:\n",
    "            print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "            print(str(e))\n",
    "        except requests.Timeout as e:\n",
    "            print(\"OOPS!! Timeout Error\")\n",
    "            print(str(e))\n",
    "        except requests.RequestException as e:\n",
    "            print(\"OOPS!! General Error\")\n",
    "            print(str(e))\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Someone closed the program\")\n",
    "\n",
    "        if '2018' in dates[0]:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
